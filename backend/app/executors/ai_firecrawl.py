"""Firecrawl AI çˆ¬è™«æ¨¡å—æ‰§è¡Œå™¨ - çº¯ Python å®ç°ï¼Œæ— éœ€å¤–éƒ¨æœåŠ¡"""
from .base import ModuleExecutor, ExecutionContext, ModuleResult, register_executor
import json
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import html2text
import re
from urllib.parse import urljoin, urlparse
from typing import List, Set, Dict
import asyncio


@register_executor
class FirecrawlScrapeExecutor(ModuleExecutor):
    """Firecrawl AI å•é¡µæ•°æ®æŠ“å–
    
    ä½¿ç”¨ Playwright + BeautifulSoup æ™ºèƒ½æå–å•ä¸ªç½‘é¡µçš„ç»“æ„åŒ–æ•°æ®ã€‚
    æ”¯æŒ Markdownã€HTMLã€çº¯æ–‡æœ¬ç­‰å¤šç§æ ¼å¼ã€‚
    """
    
    @property
    def module_type(self) -> str:
        return "firecrawl_scrape"
    
    async def execute(self, config: dict, context: ExecutionContext) -> ModuleResult:
        try:
            # è·å–é…ç½®
            url = context.resolve_value(config.get('url', ''))
            variable_name = config.get('variableName', 'scrape_result')
            
            # æŠ“å–é€‰é¡¹
            formats = config.get('formats', ['markdown'])
            only_main_content = config.get('onlyMainContent', True)
            include_tags = context.resolve_value(config.get('includeTags', ''))
            exclude_tags = context.resolve_value(config.get('excludeTags', ''))
            wait_for = context.resolve_value(config.get('waitFor', ''))
            timeout = config.get('timeout', 30000)
            
            # å‚æ•°éªŒè¯
            if not url:
                return ModuleResult(success=False, error="URL ä¸èƒ½ä¸ºç©º")
            
            # å‘é€è¿›åº¦æ—¥å¿—
            await context.send_progress(f"ğŸ”¥ æ­£åœ¨æŠ“å–é¡µé¢: {url}", "info")
            await context.send_progress(f"æ ¼å¼: {', '.join(formats)}", "info")
            
            # ä½¿ç”¨ Playwright è·å–é¡µé¢å†…å®¹
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                try:
                    # è®¿é—®é¡µé¢
                    await page.goto(url, timeout=timeout, wait_until='networkidle')
                    
                    # å¦‚æœæŒ‡å®šäº†ç­‰å¾…é€‰æ‹©å™¨
                    if wait_for:
                        try:
                            await page.wait_for_selector(wait_for, timeout=5000)
                        except:
                            pass
                    
                    # è·å– HTML å†…å®¹
                    html_content = await page.content()
                    
                    # ä½¿ç”¨ BeautifulSoup è§£æ
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in soup(['script', 'style', 'noscript']):
                        script.decompose()
                    
                    # å¦‚æœæŒ‡å®šäº†æ’é™¤æ ‡ç­¾
                    if exclude_tags:
                        exclude_list = [tag.strip() for tag in exclude_tags.split(',') if tag.strip()]
                        for tag in exclude_list:
                            for element in soup.find_all(tag):
                                element.decompose()
                    
                    # å¦‚æœåªæå–ä¸»å†…å®¹
                    if only_main_content:
                        # å°è¯•æ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸ
                        main_content = (
                            soup.find('main') or 
                            soup.find('article') or 
                            soup.find('div', {'id': re.compile(r'content|main', re.I)}) or
                            soup.find('div', {'class': re.compile(r'content|main', re.I)}) or
                            soup.body
                        )
                        if main_content:
                            soup = BeautifulSoup(str(main_content), 'html.parser')
                    
                    # å¦‚æœæŒ‡å®šäº†åŒ…å«æ ‡ç­¾
                    if include_tags:
                        include_list = [tag.strip() for tag in include_tags.split(',') if tag.strip()]
                        filtered_soup = BeautifulSoup('', 'html.parser')
                        for tag in include_list:
                            for element in soup.find_all(tag):
                                filtered_soup.append(element)
                        soup = filtered_soup
                    
                    # æ„å»ºç»“æœ
                    result = {
                        'url': url,
                        'title': soup.title.string if soup.title else '',
                    }
                    
                    # æ ¹æ®æ ¼å¼ç”Ÿæˆå†…å®¹
                    if 'markdown' in formats:
                        h = html2text.HTML2Text()
                        h.ignore_links = False
                        h.ignore_images = False
                        h.body_width = 0
                        result['markdown'] = h.handle(str(soup))
                    
                    if 'html' in formats:
                        result['html'] = str(soup)
                    
                    if 'text' in formats:
                        result['text'] = soup.get_text(separator='\n', strip=True)
                    
                    # æå–å…ƒæ•°æ®
                    result['metadata'] = {
                        'title': soup.title.string if soup.title else '',
                        'description': '',
                        'language': soup.html.get('lang', '') if soup.html else '',
                    }
                    
                    # æå– meta æ ‡ç­¾
                    meta_desc = soup.find('meta', {'name': 'description'})
                    if meta_desc:
                        result['metadata']['description'] = meta_desc.get('content', '')
                    
                finally:
                    await browser.close()
            
            # ä¿å­˜ç»“æœåˆ°å˜é‡
            context.set_variable(variable_name, result)
            
            # æ ¼å¼åŒ–ç»“æœç”¨äºæ˜¾ç¤º
            result_preview = ""
            if 'markdown' in result:
                markdown_content = result['markdown']
                if len(markdown_content) > 300:
                    result_preview = markdown_content[:300] + "..."
                else:
                    result_preview = markdown_content
            elif 'html' in result:
                html_content = result['html']
                if len(html_content) > 300:
                    result_preview = html_content[:300] + "..."
                else:
                    result_preview = html_content
            else:
                result_preview = json.dumps(result, ensure_ascii=False, indent=2)[:300] + "..."
            
            return ModuleResult(
                success=True,
                message=f"âœ… æˆåŠŸæŠ“å–é¡µé¢æ•°æ®ï¼Œå·²ä¿å­˜åˆ°å˜é‡ {variable_name}\n\né¢„è§ˆ:\n{result_preview}",
                data=result
            )
        
        except Exception as e:
            return ModuleResult(success=False, error=f"æŠ“å–å¤±è´¥: {str(e)}")


@register_executor
class FirecrawlMapExecutor(ModuleExecutor):
    """Firecrawl AI ç½‘ç«™é“¾æ¥æŠ“å–
    
    ä½¿ç”¨ Playwright æ™ºèƒ½å‘ç°ç½‘ç«™çš„æ‰€æœ‰é“¾æ¥ã€‚
    å¯ä»¥ç”¨äºæ„å»ºç½‘ç«™åœ°å›¾æˆ–æ‰¹é‡æŠ“å–ã€‚
    """
    
    @property
    def module_type(self) -> str:
        return "firecrawl_map"
    
    async def execute(self, config: dict, context: ExecutionContext) -> ModuleResult:
        try:
            # è·å–é…ç½®
            url = context.resolve_value(config.get('url', ''))
            variable_name = config.get('variableName', 'map_result')
            
            # Map é€‰é¡¹
            search = context.resolve_value(config.get('search', ''))
            include_subdomains = config.get('includeSubdomains', False)
            limit = config.get('limit', 5000)
            
            # å‚æ•°éªŒè¯
            if not url:
                return ModuleResult(success=False, error="URL ä¸èƒ½ä¸ºç©º")
            
            # å‘é€è¿›åº¦æ—¥å¿—
            await context.send_progress(f"ğŸ—ºï¸ æ­£åœ¨æŠ“å–ç½‘ç«™é“¾æ¥: {url}", "info")
            if search:
                await context.send_progress(f"æœç´¢å…³é”®è¯: {search}", "info")
            
            # è§£æåŸºç¡€ URL
            base_domain = urlparse(url).netloc
            
            # ä½¿ç”¨ Playwright è·å–é¡µé¢å¹¶æå–é“¾æ¥
            links_found: Set[str] = set()
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                try:
                    await page.goto(url, timeout=30000, wait_until='networkidle')
                    
                    # è·å–æ‰€æœ‰é“¾æ¥
                    all_links = await page.eval_on_selector_all(
                        'a[href]',
                        '(elements) => elements.map(e => e.href)'
                    )
                    
                    # è¿‡æ»¤é“¾æ¥
                    for link in all_links:
                        # è½¬æ¢ä¸ºç»å¯¹ URL
                        absolute_url = urljoin(url, link)
                        parsed = urlparse(absolute_url)
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ HTTP/HTTPS
                        if parsed.scheme not in ['http', 'https']:
                            continue
                        
                        # æ£€æŸ¥åŸŸå
                        if not include_subdomains:
                            if parsed.netloc != base_domain:
                                continue
                        else:
                            if not parsed.netloc.endswith(base_domain):
                                continue
                        
                        # å¦‚æœæœ‰æœç´¢å…³é”®è¯ï¼Œè¿‡æ»¤
                        if search and search.lower() not in absolute_url.lower():
                            continue
                        
                        links_found.add(absolute_url)
                        
                        # è¾¾åˆ°é™åˆ¶
                        if len(links_found) >= limit:
                            break
                    
                finally:
                    await browser.close()
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            links = sorted(list(links_found))
            
            # ä¿å­˜ç»“æœåˆ°å˜é‡
            context.set_variable(variable_name, links)
            
            # æ ¼å¼åŒ–ç»“æœç”¨äºæ˜¾ç¤º
            links_preview = "\n".join(links[:10])
            if len(links) > 10:
                links_preview += f"\n... è¿˜æœ‰ {len(links) - 10} ä¸ªé“¾æ¥"
            
            return ModuleResult(
                success=True,
                message=f"âœ… æˆåŠŸæŠ“å– {len(links)} ä¸ªé“¾æ¥ï¼Œå·²ä¿å­˜åˆ°å˜é‡ {variable_name}\n\né“¾æ¥é¢„è§ˆ:\n{links_preview}",
                data=links
            )
        
        except Exception as e:
            return ModuleResult(success=False, error=f"é“¾æ¥æŠ“å–å¤±è´¥: {str(e)}")


@register_executor
class FirecrawlCrawlExecutor(ModuleExecutor):
    """Firecrawl AI å…¨ç«™æ•°æ®æŠ“å–
    
    ä½¿ç”¨ Playwright æ™ºèƒ½çˆ¬å–æ•´ä¸ªç½‘ç«™çš„æ•°æ®ã€‚
    æ”¯æŒæ·±åº¦çˆ¬å–ã€æ™ºèƒ½è¿‡æ»¤ã€å¹¶å‘æ§åˆ¶ç­‰é«˜çº§åŠŸèƒ½ã€‚
    """
    
    @property
    def module_type(self) -> str:
        return "firecrawl_crawl"
    
    async def _scrape_page(self, url: str, formats: List[str], only_main_content: bool, browser) -> Dict:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        page = await browser.new_page()
        try:
            await page.goto(url, timeout=30000, wait_until='networkidle')
            html_content = await page.content()
            
            # ä½¿ç”¨ BeautifulSoup è§£æ
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(['script', 'style', 'noscript']):
                script.decompose()
            
            # å¦‚æœåªæå–ä¸»å†…å®¹
            if only_main_content:
                main_content = (
                    soup.find('main') or 
                    soup.find('article') or 
                    soup.find('div', {'id': re.compile(r'content|main', re.I)}) or
                    soup.find('div', {'class': re.compile(r'content|main', re.I)}) or
                    soup.body
                )
                if main_content:
                    soup = BeautifulSoup(str(main_content), 'html.parser')
            
            # æ„å»ºç»“æœ
            result = {
                'url': url,
                'title': soup.title.string if soup.title else '',
            }
            
            # æ ¹æ®æ ¼å¼ç”Ÿæˆå†…å®¹
            if 'markdown' in formats:
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = False
                h.body_width = 0
                result['markdown'] = h.handle(str(soup))
            
            if 'html' in formats:
                result['html'] = str(soup)
            
            if 'text' in formats:
                result['text'] = soup.get_text(separator='\n', strip=True)
            
            return result
        finally:
            await page.close()
    
    async def execute(self, config: dict, context: ExecutionContext) -> ModuleResult:
        try:
            # è·å–é…ç½®
            url = context.resolve_value(config.get('url', ''))
            variable_name = config.get('variableName', 'crawl_result')
            
            # Crawl é€‰é¡¹
            max_depth = config.get('maxDepth', 2)
            limit = config.get('limit', 100)
            include_paths = context.resolve_value(config.get('includePaths', ''))
            exclude_paths = context.resolve_value(config.get('excludePaths', ''))
            allow_external_links = config.get('allowExternalLinks', False)
            
            # æŠ“å–æ ¼å¼
            formats = config.get('formats', ['markdown'])
            only_main_content = config.get('onlyMainContent', True)
            
            # å‚æ•°éªŒè¯
            if not url:
                return ModuleResult(success=False, error="URL ä¸èƒ½ä¸ºç©º")
            
            # å‘é€è¿›åº¦æ—¥å¿—
            await context.send_progress(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–æ•´ä¸ªç½‘ç«™: {url}", "info")
            await context.send_progress(f"æœ€å¤§æ·±åº¦: {max_depth}, é¡µé¢é™åˆ¶: {limit}", "info")
            
            # è§£æåŸºç¡€ URL
            base_domain = urlparse(url).netloc
            base_url = f"{urlparse(url).scheme}://{base_domain}"
            
            # å‡†å¤‡è¿‡æ»¤è§„åˆ™
            include_patterns = []
            if include_paths:
                include_patterns = [p.strip() for p in include_paths.split(',') if p.strip()]
            
            exclude_patterns = []
            if exclude_paths:
                exclude_patterns = [p.strip() for p in exclude_paths.split(',') if p.strip()]
            
            # çˆ¬å–çŠ¶æ€
            visited: Set[str] = set()
            to_visit: List[tuple] = [(url, 0)]  # (url, depth)
            results: List[Dict] = []
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                
                try:
                    while to_visit and len(results) < limit:
                        current_url, depth = to_visit.pop(0)
                        
                        # è·³è¿‡å·²è®¿é—®çš„
                        if current_url in visited:
                            continue
                        
                        visited.add(current_url)
                        
                        # æ£€æŸ¥æ·±åº¦
                        if depth > max_depth:
                            continue
                        
                        # æ£€æŸ¥åŒ…å«/æ’é™¤è§„åˆ™
                        if include_patterns:
                            if not any(pattern in current_url for pattern in include_patterns):
                                continue
                        
                        if exclude_patterns:
                            if any(pattern in current_url for pattern in exclude_patterns):
                                continue
                        
                        await context.send_progress(f"â³ æ­£åœ¨çˆ¬å– ({len(results)+1}/{limit}): {current_url}", "info")
                        
                        try:
                            # æŠ“å–é¡µé¢
                            page_data = await self._scrape_page(current_url, formats, only_main_content, browser)
                            results.append(page_data)
                            
                            # å¦‚æœè¿˜æ²¡è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œæå–é“¾æ¥
                            if depth < max_depth and len(results) < limit:
                                page = await browser.new_page()
                                try:
                                    await page.goto(current_url, timeout=30000)
                                    links = await page.eval_on_selector_all(
                                        'a[href]',
                                        '(elements) => elements.map(e => e.href)'
                                    )
                                    
                                    for link in links:
                                        absolute_url = urljoin(current_url, link)
                                        parsed = urlparse(absolute_url)
                                        
                                        # æ£€æŸ¥æ˜¯å¦æ˜¯ HTTP/HTTPS
                                        if parsed.scheme not in ['http', 'https']:
                                            continue
                                        
                                        # æ£€æŸ¥åŸŸå
                                        if not allow_external_links:
                                            if parsed.netloc != base_domain:
                                                continue
                                        
                                        # æ·»åŠ åˆ°å¾…è®¿é—®åˆ—è¡¨
                                        if absolute_url not in visited:
                                            to_visit.append((absolute_url, depth + 1))
                                
                                finally:
                                    await page.close()
                        
                        except Exception as e:
                            await context.send_progress(f"âš ï¸ è·³è¿‡é¡µé¢ {current_url}: {str(e)}", "warning")
                            continue
                
                finally:
                    await browser.close()
            
            # ä¿å­˜ç»“æœåˆ°å˜é‡
            context.set_variable(variable_name, results)
            
            # æ ¼å¼åŒ–ç»“æœç”¨äºæ˜¾ç¤º
            result_summary = f"æˆåŠŸçˆ¬å– {len(results)} ä¸ªé¡µé¢"
            
            if len(results) > 0:
                first_page = results[0]
                if 'markdown' in first_page:
                    preview = first_page['markdown'][:200] + "..."
                    result_summary += f"\n\nç¬¬ä¸€ä¸ªé¡µé¢é¢„è§ˆ:\n{preview}"
            
            return ModuleResult(
                success=True,
                message=f"âœ… {result_summary}ï¼Œå·²ä¿å­˜åˆ°å˜é‡ {variable_name}",
                data=results
            )
        
        except Exception as e:
            return ModuleResult(success=False, error=f"å…¨ç«™çˆ¬å–å¤±è´¥: {str(e)}")
