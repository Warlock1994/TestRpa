"""AIæ™ºèƒ½çˆ¬è™«æ¨¡å—æ‰§è¡Œå™¨ - åŸºäº ScrapeGraphAI"""
from .base import ModuleExecutor, ExecutionContext, ModuleResult, register_executor
import json

# å°è¯•å¯¼å…¥ nest_asyncio ä»¥æ”¯æŒåµŒå¥—äº‹ä»¶å¾ªç¯
try:
    import nest_asyncio
    nest_asyncio.apply()
except ImportError:
    pass  # å¦‚æœæœªå®‰è£…ï¼Œç»§ç»­æ‰§è¡Œï¼ˆå¯èƒ½ä¼šé‡åˆ°äº‹ä»¶å¾ªç¯é—®é¢˜ï¼‰


@register_executor
class AISmartScraperExecutor(ModuleExecutor):
    """AIæ™ºèƒ½çˆ¬è™«æ¨¡å—æ‰§è¡Œå™¨
    
    ä½¿ç”¨ ScrapeGraphAI çš„èƒ½åŠ›ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥æå–ç½‘é¡µæ•°æ®ã€‚
    ä¼˜ç‚¹ï¼šé€‚åº”ç½‘é¡µç»“æ„å˜åŒ–ï¼Œå‡å°‘ç»´æŠ¤æˆæœ¬
    ç¼ºç‚¹ï¼šé€Ÿåº¦æ¯”ä¼ ç»Ÿçˆ¬è™«æ…¢ï¼Œéœ€è¦ LLM æ”¯æŒ
    """
    
    @property
    def module_type(self) -> str:
        return "ai_smart_scraper"
    
    async def execute(self, config: dict, context: ExecutionContext) -> ModuleResult:
        try:
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† scrapegraphai
            try:
                from scrapegraphai.graphs import SmartScraperGraph
            except ImportError:
                return ModuleResult(
                    success=False, 
                    error="æœªå®‰è£… scrapegraphai åº“ã€‚è¯·è¿è¡Œ: pip install scrapegraphai"
                )
            
            # è·å–é…ç½®
            url = context.resolve_value(config.get('url', ''))
            prompt = context.resolve_value(config.get('prompt', ''))
            variable_name = config.get('variableName', '')
            wait_time = config.get('waitTime', 3)  # é»˜è®¤ç­‰å¾…3ç§’
            
            # LLM é…ç½®
            llm_provider = context.resolve_value(config.get('llmProvider', 'ollama'))
            api_url = context.resolve_value(config.get('apiUrl', ''))
            llm_model = context.resolve_value(config.get('llmModel', 'llama3.2'))
            api_key = context.resolve_value(config.get('apiKey', ''))
            
            # å‚æ•°éªŒè¯
            if not url:
                return ModuleResult(success=False, error="URL ä¸èƒ½ä¸ºç©º")
            
            if not prompt:
                return ModuleResult(success=False, error="æå–æç¤ºè¯ä¸èƒ½ä¸ºç©º")
            
            if not variable_name:
                return ModuleResult(success=False, error="å˜é‡åä¸èƒ½ä¸ºç©º")
            
            # å¦‚æœæœ‰æ‰“å¼€çš„æµè§ˆå™¨é¡µé¢ï¼Œä½¿ç”¨é¡µé¢å†…å®¹ï¼›å¦åˆ™ä½¿ç”¨ URL
            html_content = None
            if context.page is not None:
                try:
                    # å°è¯•è·å–å½“å‰é¡µé¢çš„ HTML å†…å®¹
                    await context.switch_to_latest_page()
                    current_url = context.page.url
                    # å¦‚æœå½“å‰é¡µé¢çš„ URL ä¸ç›®æ ‡ URL åŒ¹é…ï¼Œä½¿ç”¨é¡µé¢å†…å®¹
                    if url in current_url or current_url in url:
                        html_content = await context.page.content()
                        await context.send_progress(f"ä½¿ç”¨å·²æ‰“å¼€çš„æµè§ˆå™¨é¡µé¢å†…å®¹", "info")
                except Exception as e:
                    # å¦‚æœè·å–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ URL
                    await context.send_progress(f"æ— æ³•è·å–æµè§ˆå™¨é¡µé¢å†…å®¹ï¼Œå°†ç›´æ¥è®¿é—® URL", "info")
            
            # æ„å»º LLM é…ç½®
            llm_config = {
                "max_tokens": 8192,
            }
            
            if llm_provider == 'ollama':
                llm_config["model"] = f"ollama/{llm_model}"
                llm_config["format"] = "json"  # Ollama æ”¯æŒ format å‚æ•°
            elif llm_provider in ['openai', 'zhipu', 'deepseek', 'custom']:
                # OpenAI å…¼å®¹çš„ API - ScrapeGraphAI éœ€è¦ openai/ å‰ç¼€
                if not api_key:
                    return ModuleResult(success=False, error=f"ä½¿ç”¨ {llm_provider} éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"openai/{llm_model}"  # ä½¿ç”¨ openai/ å‰ç¼€
                # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ API URLï¼Œä½¿ç”¨å®ƒ
                if api_url:
                    # ScrapeGraphAI ä¼šè‡ªåŠ¨æ·»åŠ  /chat/completionsï¼Œæ‰€ä»¥å¦‚æœ URL å·²åŒ…å«ï¼Œéœ€è¦å»æ‰
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'groq':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Groq éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"groq/{llm_model}"
                if api_url:
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'gemini':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Gemini éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"gemini/{llm_model}"
                if api_url:
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'azure':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Azure éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"azure/{llm_model}"
                # Azure å¯èƒ½éœ€è¦é¢å¤–é…ç½®
                azure_endpoint = context.resolve_value(config.get('azureEndpoint', ''))
                if azure_endpoint:
                    llm_config["azure_endpoint"] = azure_endpoint
            
            # æ„å»ºå®Œæ•´é…ç½®
            graph_config = {
                "llm": llm_config,
                "verbose": config.get('verbose', False),
                "headless": config.get('headless', True),
            }
            
            # å‘é€è¿›åº¦æ—¥å¿—
            await context.send_progress(f"æ­£åœ¨ä½¿ç”¨ AI æ™ºèƒ½çˆ¬è™«æå–æ•°æ®...", "info")
            await context.send_progress(f"LLM: {llm_provider}/{llm_model}", "info")
            await context.send_progress(f"URL: {url}", "info")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            if wait_time > 0:
                await context.send_progress(f"â³ ç­‰å¾…é¡µé¢åŠ è½½ {wait_time} ç§’...", "info")
                import asyncio
                await asyncio.sleep(wait_time)
            
            # ä½¿ç”¨ HTML å†…å®¹æˆ– URL ä½œä¸ºæ•°æ®æº
            source = html_content if html_content else url
            
            # åˆ›å»ºæ™ºèƒ½çˆ¬è™«
            smart_scraper = SmartScraperGraph(
                prompt=prompt,
                source=source,
                config=graph_config
            )
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆnest_asyncio å…è®¸åœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œï¼‰
            result = smart_scraper.run()
            
            # ä¿å­˜ç»“æœåˆ°å˜é‡
            context.set_variable(variable_name, result)
            
            # æ ¼å¼åŒ–ç»“æœç”¨äºæ˜¾ç¤º
            result_str = json.dumps(result, ensure_ascii=False, indent=2)
            if len(result_str) > 500:
                result_preview = result_str[:500] + "..."
            else:
                result_preview = result_str
            
            return ModuleResult(
                success=True,
                message=f"AI æ™ºèƒ½çˆ¬è™«æˆåŠŸæå–æ•°æ®ï¼Œå·²ä¿å­˜åˆ°å˜é‡ {variable_name}",
                data=result_preview
            )
        
        except Exception as e:
            error_msg = str(e)
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯æç¤º
            if "ollama" in error_msg.lower():
                error_msg += "\næç¤ºï¼šè¯·ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œï¼Œä¸”å·²ä¸‹è½½å¯¹åº”æ¨¡å‹"
            elif "api" in error_msg.lower() and "key" in error_msg.lower():
                error_msg += "\næç¤ºï¼šè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®"
            
            return ModuleResult(success=False, error=f"AI æ™ºèƒ½çˆ¬è™«å¤±è´¥: {error_msg}")


@register_executor
class AIElementSelectorExecutor(ModuleExecutor):
    """AIæ™ºèƒ½å…ƒç´ é€‰æ‹©å™¨æ¨¡å—æ‰§è¡Œå™¨
    
    ä½¿ç”¨ AI èƒ½åŠ›ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥è·å–å…ƒç´ çš„ Selectorã€‚
    å³ä½¿ç½‘é¡µç»“æ„å˜åŒ–ï¼Œä¹Ÿèƒ½å‡†ç¡®æ‰¾åˆ°å…ƒç´ ã€‚
    """
    
    @property
    def module_type(self) -> str:
        return "ai_element_selector"
    
    async def execute(self, config: dict, context: ExecutionContext) -> ModuleResult:
        try:
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† scrapegraphai
            try:
                from scrapegraphai.graphs import SmartScraperGraph
            except ImportError:
                return ModuleResult(
                    success=False, 
                    error="æœªå®‰è£… scrapegraphai åº“ã€‚è¯·è¿è¡Œ: pip install scrapegraphai"
                )
            
            import json  # ç§»åˆ°é¡¶éƒ¨ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
            
            # è·å–é…ç½®
            url = context.resolve_value(config.get('url', ''))
            element_description = context.resolve_value(config.get('elementDescription', ''))
            variable_name = config.get('variableName', '')
            wait_time = config.get('waitTime', 3)  # é»˜è®¤ç­‰å¾…3ç§’
            
            # LLM é…ç½®
            llm_provider = context.resolve_value(config.get('llmProvider', 'ollama'))
            api_url = context.resolve_value(config.get('apiUrl', ''))
            llm_model = context.resolve_value(config.get('llmModel', 'llama3.2'))
            api_key = context.resolve_value(config.get('apiKey', ''))
            
            # å‚æ•°éªŒè¯
            if not url:
                return ModuleResult(success=False, error="URL ä¸èƒ½ä¸ºç©º")
            
            if not element_description:
                return ModuleResult(success=False, error="å…ƒç´ æè¿°ä¸èƒ½ä¸ºç©º")
            
            if not variable_name:
                return ModuleResult(success=False, error="å˜é‡åä¸èƒ½ä¸ºç©º")
            
            # æ„å»º LLM é…ç½®
            llm_config = {
                "max_tokens": 8192,
            }
            
            if llm_provider == 'ollama':
                llm_config["model"] = f"ollama/{llm_model}"
                llm_config["format"] = "json"  # Ollama æ”¯æŒ format å‚æ•°
            elif llm_provider in ['openai', 'zhipu', 'deepseek', 'custom']:
                # OpenAI å…¼å®¹çš„ API - ScrapeGraphAI éœ€è¦ openai/ å‰ç¼€
                if not api_key:
                    return ModuleResult(success=False, error=f"ä½¿ç”¨ {llm_provider} éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"openai/{llm_model}"  # ä½¿ç”¨ openai/ å‰ç¼€
                # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ API URLï¼Œä½¿ç”¨å®ƒ
                if api_url:
                    # ScrapeGraphAI ä¼šè‡ªåŠ¨æ·»åŠ  /chat/completionsï¼Œæ‰€ä»¥å¦‚æœ URL å·²åŒ…å«ï¼Œéœ€è¦å»æ‰
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'groq':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Groq éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"groq/{llm_model}"
                if api_url:
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'gemini':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Gemini éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"gemini/{llm_model}"
                if api_url:
                    if api_url.endswith('/chat/completions'):
                        api_url = api_url.rsplit('/chat/completions', 1)[0]
                    llm_config["base_url"] = api_url
            elif llm_provider == 'azure':
                if not api_key:
                    return ModuleResult(success=False, error="ä½¿ç”¨ Azure éœ€è¦æä¾› API Key")
                llm_config["api_key"] = api_key
                llm_config["model"] = f"azure/{llm_model}"
                azure_endpoint = context.resolve_value(config.get('azureEndpoint', ''))
                if azure_endpoint:
                    llm_config["azure_endpoint"] = azure_endpoint
            
            # æ„å»ºå®Œæ•´é…ç½®
            graph_config = {
                "llm": llm_config,
                "verbose": config.get('verbose', False),
                "headless": True,
            }
            
            # æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚è¿”å› CSS é€‰æ‹©å™¨
            prompt = f"""
è¯·åˆ†æè¿™ä¸ªç½‘é¡µï¼Œæ‰¾åˆ°ç¬¦åˆä»¥ä¸‹æè¿°çš„å…ƒç´ ï¼š{element_description}

è¯·è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- selector: è¯¥å…ƒç´ çš„ CSS é€‰æ‹©å™¨ï¼ˆå°½å¯èƒ½ç®€æ´ä¸”å”¯ä¸€ï¼‰
- description: å¯¹æ‰¾åˆ°çš„å…ƒç´ çš„ç®€çŸ­æè¿°
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-100ï¼‰

å¦‚æœæ‰¾åˆ°å¤šä¸ªåŒ¹é…çš„å…ƒç´ ï¼Œè¿”å›æœ€å¯èƒ½çš„é‚£ä¸ªã€‚
"""
            
            # å‘é€è¿›åº¦æ—¥å¿—
            await context.send_progress(f"æ­£åœ¨ä½¿ç”¨ AI æ™ºèƒ½æŸ¥æ‰¾å…ƒç´ ...", "info")
            await context.send_progress(f"URL: {url}", "info")
            await context.send_progress(f"å…ƒç´ æè¿°: {element_description}", "info")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            if wait_time > 0:
                await context.send_progress(f"â³ ç­‰å¾…é¡µé¢åŠ è½½ {wait_time} ç§’...", "info")
                import asyncio
                await asyncio.sleep(wait_time)
            
            # åˆ›å»ºæ™ºèƒ½çˆ¬è™« - ä½¿ç”¨ URL
            smart_scraper = SmartScraperGraph(
                prompt=prompt,
                source=url,  # ä½¿ç”¨ URL
                config=graph_config
            )
            
            # æ‰§è¡ŒæŸ¥æ‰¾ï¼ˆnest_asyncio å…è®¸åœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œï¼‰
            result = smart_scraper.run()
            
            # å¦‚æœå¼€å¯è¯¦ç»†æ—¥å¿—ï¼Œæ˜¾ç¤ºåŸå§‹è¿”å›ç»“æœ
            if config.get('verbose', False):
                await context.send_progress(f"ğŸ“Š AIåŸå§‹è¿”å›: {json.dumps(result, ensure_ascii=False)}", "info")
            
            # æå–é€‰æ‹©å™¨
            selector = None
            confidence = 0
            description = ""
            
            # ScrapeGraphAI å¯èƒ½è¿”å›ä¸åŒçš„ç»“æ„
            if isinstance(result, dict):
                # å°è¯•ä» content å­—æ®µè·å–ï¼ˆæ–°ç‰ˆæœ¬æ ¼å¼ï¼‰
                if 'content' in result and isinstance(result['content'], dict):
                    content = result['content']
                    selector = content.get('selector')
                    confidence = content.get('confidence', 0)
                    description = content.get('description', '')
                # æˆ–è€…ç›´æ¥ä»é¡¶å±‚è·å–ï¼ˆæ—§ç‰ˆæœ¬æ ¼å¼ï¼‰
                else:
                    selector = result.get('selector')
                    confidence = result.get('confidence', 0)
                    description = result.get('description', '')
            elif isinstance(result, str):
                selector = result
                confidence = 50
                description = "AI è¿”å›çš„é€‰æ‹©å™¨"
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›äº† NAï¼ˆè¡¨ç¤ºæœªæ‰¾åˆ°ï¼‰
            if not selector or selector == 'NA':
                return ModuleResult(
                    success=False,
                    error=f"AI æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„å…ƒç´ ã€‚\nåŸå› ï¼šScrapeGraphAI è·å–ä¸åˆ°ç½‘é¡µå†…å®¹ï¼ˆå¯èƒ½æ˜¯åçˆ¬ã€åŠ è½½è¶…æ—¶æˆ– JS æ¸²æŸ“é—®é¢˜ï¼‰\nè¿”å›ç»“æœ: {json.dumps(result, ensure_ascii=False)}\n\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ä¼ ç»Ÿçš„'è·å–å…ƒç´ åˆ—è¡¨'ç­‰æ¨¡å—ï¼Œæˆ–ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰æ‰‹åŠ¨è·å–é€‰æ‹©å™¨"
                )
            
            # æ£€æŸ¥æè¿°ä¸­æ˜¯å¦æåˆ°"å†…å®¹ä¸ºç©º"
            if description and ('å†…å®¹ä¸ºç©º' in description or 'æ— æ³•è¿›è¡Œå®é™…åˆ†æ' in description or 'æ— æ³•è¿›è¡Œå…·ä½“åˆ†æ' in description):
                await context.send_progress(f"âš ï¸ è­¦å‘Šï¼šAI æç¤ºç½‘ç«™å†…å®¹ä¸ºç©ºï¼Œè¿”å›çš„é€‰æ‹©å™¨å¯èƒ½ä¸å‡†ç¡®", "warning")
            
            # ä¿å­˜é€‰æ‹©å™¨åˆ°å˜é‡
            context.set_variable(variable_name, selector)
            
            return ModuleResult(
                success=True,
                message=f"AI æˆåŠŸæ‰¾åˆ°å…ƒç´ é€‰æ‹©å™¨: {selector}\næè¿°: {description}\nç½®ä¿¡åº¦: {confidence}%",
                data=selector
            )
        
        except Exception as e:
            error_msg = str(e)
            if "ollama" in error_msg.lower():
                error_msg += "\næç¤ºï¼šè¯·ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œï¼Œä¸”å·²ä¸‹è½½å¯¹åº”æ¨¡å‹"
            elif "api" in error_msg.lower() and "key" in error_msg.lower():
                error_msg += "\næç¤ºï¼šè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®"
            
            return ModuleResult(success=False, error=f"AI æ™ºèƒ½å…ƒç´ é€‰æ‹©å¤±è´¥: {error_msg}")
